---
title: Fan District Association
---

The City of Richmond maintains an arcgis geodata repository called [Richmond GeoHub](https://richmond-geo-hub-cor.hub.arcgis.com).

Data on the geohub are organized into key areas.  For our analysis, we'll be using the 
following data sources.

[Addresses](https://richmond-geo-hub-cor.hub.arcgis.com/datasets/674d645c444f4191998f0ebb96e56047_0/explore?location=37.527383%2C-77.493413%2C10.99)
: All of the official, mapped inventory of all unit and non-unit-based addresses in the City. Includes only active addresses.

[Parcels](https://richmond-geo-hub-cor.hub.arcgis.com/datasets/fbfce2aab2a44c05bc0abc2d6ea7e54a_0/explore?location=37.525465%2C-77.493422%2C10.60)
: City of Richmond property ownership information, mapped by land ownership (parcels).

[Civic Associations](https://richmond-geo-hub-cor.hub.arcgis.com/datasets/be39ce592f3e4419babe11d1b967e2f3_0/explore?location=37.528836%2C-77.494197%2C10.96)
: Represents civic organization boundaries in the city of Richmond, Virginia.

[National Historic Districts](https://richmond-geo-hub-cor.hub.arcgis.com/datasets/38bd0df47c6440528c2ef22daaf81883_0/explore?location=37.550339%2C-77.468606%2C14.93)
: Represents districts and sites that are listed on the National Register of Historic Places (Federal designation) and the Virginia Landmarks Register (State designation).

[Neighborhoods](https://richmond-geo-hub-cor.hub.arcgis.com/datasets/7a0ffef23d16461e9728c065f27b2790_0/explore?location=37.525021%2C-77.493427%2C10.73)
: City of Richmond Neighborhoods.

For our Fan District analysis we will be working with [Civic Associations](https://richmond-geo-hub-cor.hub.arcgis.com/datasets/be39ce592f3e4419babe11d1b967e2f3_0/explore?location=37.528836%2C-77.494197%2C10.96)
to get the formal boundary of the *Fan District Association*.

We then use that boundary to determine [Addresses](https://richmond-geo-hub-cor.hub.arcgis.com/datasets/674d645c444f4191998f0ebb96e56047_0/explore?location=37.527383%2C-77.493413%2C10.99)
and [Parcels](https://richmond-geo-hub-cor.hub.arcgis.com/datasets/fbfce2aab2a44c05bc0abc2d6ea7e54a_0/explore?location=37.525465%2C-77.493422%2C10.60)
in the *Fan District Association*

```{python}


import os
import re
import sys
import sqlite3
import requests
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

from matplotlib.patches import Patch
from matplotlib.lines import Line2D
from math import floor
from loguru import logger

sys.path.append("..")
from fandu.geo_utils import get_newest_feature_file

pd.set_option("display.max_rows", None)

geojson_folder = "../precious/"
features = ["Addresses","Parcels"]
selector = "Civic_Associations"
selector_key = "Fan District Association"

```

# Fan District Association

```{python}

# Load the neighborhoods GeoJSON
data = {}
for feature in [selector] + features:
    geofile = get_newest_feature_file( geojson_folder, feature )
    #logger.debug(geofile)
    data[feature] = gpd.read_file( geofile )

for feature in features:
    data[feature] = data[feature].to_crs( data[selector].crs )


# columns to drop:

shared_drops = ['CreatedBy','CreatedDate','EditBy','EditDate']
drop_columns = {
    "Civic_Associations" : ['OBJECTID'] + shared_drops,
    "Addresses"          : ['OBJECTID'] + shared_drops,
    "Parcels"            : ['OBJECTID'],
}

for feature in [selector] + features:
    data[feature] = data[feature].drop(columns=drop_columns[feature])

```


```{python}

for feature in features:
    data[selector_key] = data[selector][ data[selector]["Name"] == selector_key ]

for feature in features:
    predicate = "overlaps" if feature=="Neighborhoods" else "within"
    data[feature+"_in_fan"] = gpd.sjoin(data[feature], data[selector_key], predicate=predicate, how="inner")

for feature in features:
    feature_name = feature+"_in_fan"
    data[feature_name] = data[feature_name].drop(columns=["index_right","AdoptionDate","ChangeDate"])

for feature in features:
    feature_name = feature+"_in_fan"
    df = data[feature_name].drop(columns="geometry")
    df.to_csv(f"{feature_name}.csv", index=False)



```

## FDA - Key measures


### Area 
```{python}

# Get the Shape_Area value for the row where Name == selector_key
projected = data[selector].to_crs(epsg=2283)
# Get Shape_Area for the selected feature
area_sqft = projected.loc[projected["Name"] == selector_key, "geometry"].area.iloc[0]

# Convert to acres
area_acres = area_sqft / 43560
area_sqmi = area_sqft / 27_878_400
area_sqm = area_sqft * 0.09290304
area_hectare = area_sqm / 10_000
area_sqkm = area_sqm / 1_000_000

print(f"{selector_key}: Area: {area_sqft:,.0f} sq.ft.,  {area_acres:,.2f} acres, {area_sqmi:,.3f} sq.miles")
print(f"{selector_key}: Area: {area_sqm:,.0f} sq.m, {area_hectare:,.3f} hectares, {area_sqkm:,.3f} sq.km")

```

### Examples of records sharing the same AddressLabel

```{python}
feature_name = "Addresses_in_fan"
gdf = data[feature_name]

# Step 1: Find duplicated AddressLabels
duplicates = gdf[gdf.duplicated(subset=["AddressLabel"], keep=False)]

# Step 2: Group and inspect (optional)
grouped = duplicates.sort_values("AddressLabel").groupby("AddressLabel")

# Step 3: Print summary
print(f"Total duplicated AddressLabel records: {len(duplicates)}")
print(f"Number of unique duplicated labels: {duplicates['AddressLabel'].nunique()}")

# Step 4: Show some examples
print("\nExamples of duplicated AddressLabels:")
print(duplicates[["AddressLabel", "UnitType", "UnitValue"]].head(10))

```

### Examples of non-unit addresses with unit parts in AddressLabel

```{python}
feature_name = "Addresses_in_fan"
gdf = data[feature_name].copy()

# Step 1: Build a list of distinct, non-null UnitTypes
potential_unit_types = gdf["UnitType"].dropna().astype(str).str.strip().unique().tolist()
potential_unit_types = [ut for ut in potential_unit_types if ut]  # Remove blanks

# Step 2: Build regex pattern to match unit suffix at end of address
#unit_pattern = r"\b(" + "|".join(map(re.escape, potential_unit_types)) + r")\s+\S+$"
unit_pattern = r"\b(?:{})\s+\S+$".format("|".join(map(re.escape, potential_unit_types)))

# Step 3: Find rows with UnitType == None and AddressLabel contains a recognizable UnitType
mask = gdf["UnitType"].isnull() & gdf["AddressLabel"].astype(str).str.contains(unit_pattern, regex=True, na=False)

# Subset matching rows
candidates = gdf[mask].copy()

# Step 4: Report
print(f"üîç Found {len(candidates)} rows where AddressLabel ends with a recognizable UnitType + value and UnitType is missing.")
print("\nüß™ Sample rows:")
print(candidates[["AddressLabel", "UnitType", "UnitValue"]].head())

```

### Fix these issues found above.

```{python}

# Step 5: Extract UnitType and UnitValue using regex
# Example match: "501 N Arthur Ashe Blvd Unit 2" ‚Üí ("Unit", "2")
unit_extract_pattern = r"\b(" + "|".join(map(re.escape, potential_unit_types)) + r")\s+(\S+)$"
unit_parts = candidates["AddressLabel"].astype(str).str.extract(unit_extract_pattern)

# Assign back to the original gdf using index alignment
gdf.loc[candidates.index, "UnitType"] = unit_parts[0]
gdf.loc[candidates.index, "UnitValue"] = unit_parts[1]

# Step 6: Update data structure
data[feature_name] = gdf

print(f"\n‚úÖ Extracted UnitType and UnitValue for {len(unit_parts.dropna())} rows.")
```

### Examples of non-unit addreses sharing the same long/lat

```{python}

feature_name = "Addresses_in_fan"
gdf = data[feature_name]

# Step 1: Filter for base addresses (UnitType is None) and valid coordinates
base_addrs = gdf[
    gdf["UnitType"].isnull() &
    gdf["Longitude"].notnull() &
    gdf["Latitude"].notnull()
].copy()

# Step 2: Find duplicate coordinate records
dup_coords = base_addrs[
    base_addrs.duplicated(subset=["Longitude", "Latitude"], keep=False)
]

# Step 3: Group duplicates for inspection
coord_groups = dup_coords.groupby(["Latitude", "Longitude"])

# Step 4: Summary counts
print(f"üìç Total base address rows with duplicate coordinates: {len(dup_coords)}")
print(f"üîÅ Unique (lat, lon) pairs with duplicates: {coord_groups.ngroups}")

# Step 5: Show sample groups
print("\nüîç Example duplicates (UnitType is None, by lat/lon):")
for (lat, lon), group in coord_groups:
    print(f"\nCoordinates: ({lat}, {lon}) ‚Äî {len(group)} records")
    sorted_group = group.sort_values("AddressLabel")
    print(sorted_group[["AddressLabel", "UnitType", "UnitValue", "Latitude", "Longitude"]])
    break  # Remove this if you want to show all groups

```

```{python}
if 0:
    print("\n\n\nüîç First record from each duplicate (UnitType is None, by lat/lon):")
    for (lat, lon), group in coord_groups:
        first = group.sort_values("AddressLabel").iloc[0]
        print(f"{first['AddressLabel']} | UnitType={first['UnitType']} | UnitValue={first['UnitValue']} | ({lat}, {lon})")

print("\nüîç First record from each duplicate (UnitType is None, by lat/lon):")
for (lat, lon), group in coord_groups:
    first = group.sort_values("AddressLabel").iloc[0]
    group_size = len(group)
    print(f"{first['AddressLabel']} | {group_size} records | ({lat}, {lon})")


```

### Fix: jitter addresses

```{python}
# Make a copy so we can safely modify coordinates

from shapely.geometry import Point

gdf_jittered = gdf.copy()

# Define base jitter radius in degrees (very small ~meters)
base_radius_deg = 0.00001

# Loop over each group
for (lat, lon), group in coord_groups:
    count = len(group)
    if count == 1:
        continue  # No need to jitter

    radius = base_radius_deg * np.sqrt(1.0 * count)  # Increase radius with count
     # One random multiplier [0, 1) for each point
    random_scalars = np.random.rand(count)

    # Final radius for each point
    radii = radius * random_scalars
    angles = np.linspace(0, 2 * np.pi, count, endpoint=False)

    jittered_lats = lat + radii * np.sin(angles)
    jittered_lons = lon + radii * np.cos(angles)

    # Assign jittered coordinates back using index
    gdf_jittered.loc[group.index, "Latitude"] = jittered_lats
    gdf_jittered.loc[group.index, "Longitude"] = jittered_lons

data[feature_name].loc[gdf_jittered.index, "Latitude"] = gdf_jittered["Latitude"]
data[feature_name].loc[gdf_jittered.index, "Longitude"] = gdf_jittered["Longitude"]

# Update all the geometries
data[feature_name]["geometry"] = data[feature_name].apply(
    lambda row: Point(row["Longitude"], row["Latitude"]), axis=1
)

print("‚úÖ Jittered coordinates applied to duplicate lat/lon groups.")
```


### Example: Unit addresses without non-unit base address


```{python}

feature_name = "Addresses_in_fan"
gdf = data[feature_name]

# Step 1: Get unit addresses (where UnitType is not null)
unit_addrs = gdf[gdf["UnitType"].notnull()].copy()

# Step 2: Extract base address (remove unit from AddressLabel)
# Assumes unit info appears after a comma, e.g., "123 Main St, Apt 2B"

unit_addrs["BaseAddressLabel"] = (
    unit_addrs["BuildingNumber"].fillna("") + " " +
    unit_addrs["StreetDirection"].fillna("") + " " +
    unit_addrs["StreetName"].fillna("") + " " +
    unit_addrs["StreetType"].fillna("")
).str.replace(r"\s+", " ", regex=True).str.strip()

# Step 3: Get base addresses in the dataset (with UnitType == None)
base_addrs = gdf[gdf["UnitType"].isnull()].copy()
base_addrs["BaseAddressLabel"] = (
    base_addrs["BuildingNumber"].fillna("") + " " +
    base_addrs["StreetDirection"].fillna("") + " " +
    base_addrs["StreetName"].fillna("") + " " +
    base_addrs["StreetType"].fillna("")
).str.strip().replace(r"\s+", " ", regex=True)

base_addrs["AddressLabel"] = base_addrs["AddressLabel"].fillna("").astype(str).str.strip()

# Step 4: Get the set of base labels from base_addrs
base_set = set(base_addrs["BaseAddressLabel"])

# Step 5: Find unit addresses whose base is missing
missing = unit_addrs[~unit_addrs["BaseAddressLabel"].isin(base_set)]

# Step 6: Print summary
print(f"Total unit addresses: {len(unit_addrs)}")
print(f"Unit addresses with missing base address: {len(missing)}")

if not missing.empty:
    print("\nExamples of missing base addresses:")
    print(missing[["AddressLabel", "BaseAddressLabel", "UnitType", "UnitValue"]].head())

```

### List of non-unit addresses where baseaddresslabel <> addressLabel

```{python}
# Count total base records
total_base = len(base_addrs)

# Compare BaseAddressLabel to AddressLabel
match = base_addrs[base_addrs["BaseAddressLabel"] == base_addrs["AddressLabel"]]
mismatch = base_addrs[base_addrs["BaseAddressLabel"] != base_addrs["AddressLabel"]]

# Print counts
print(f"Total base addresses (UnitType is null): {total_base}")
print(f"Matching AddressLabel == BaseAddressLabel: {len(match)}")
print(f"Non-matching AddressLabel != BaseAddressLabel: {len(mismatch)}")

# Show examples of mismatches
if not mismatch.empty:
    print("\nExamples of mismatched base addresses:")
    print(mismatch[["AddressLabel", "BaseAddressLabel"]].head())

```

### Addresses with the top 20 unit counts.

```{python}

feature_name = "Addresses_in_fan"
gdf = data[feature_name].copy()

# Step 1: Create BaseAddressLabel for all addresses
gdf["BaseAddressLabel"] = (
    gdf["BuildingNumber"].fillna("") + " " +
    gdf["StreetDirection"].fillna("") + " " +
    gdf["StreetName"].fillna("") + " " +
    gdf["StreetType"].fillna("")
).str.strip().replace(r'\s+', ' ', regex=True)

# Step 2: Count number of unit addresses per BaseAddressLabel
unit_counts = (
    gdf[gdf["UnitType"].notnull()]
    .groupby("BaseAddressLabel")
    .size()
    .rename("UnitCountForBase")
)

# Step 3: Map unit counts to base addresses
gdf["UnitCount"] = gdf["BaseAddressLabel"].map(unit_counts).fillna(1).astype(int)

# Save back to data object
data[feature_name] = gdf

```

```{python}
feature_name = "Addresses_in_fan"
gdf = data[feature_name]

# Sort by UnitCount descending and drop duplicates to avoid listing each unit individually
top_bases = (
    gdf[gdf["UnitType"].isnull()]  # Only base addresses
    .sort_values("UnitCount", ascending=False)
    .head(20)
)

# Display relevant info
print(top_bases[["AddressLabel", "BaseAddressLabel", "UnitCount","UnitType"]])
```


```{python}

if 1:
    # Connect to or create a database file
    conn = sqlite3.connect("fda-data.sqlite")

    for feature in [selector]+features:

    # Write the DataFrame to a new table (overwrite if it exists)
        temp_copy = data[feature].copy()
        temp_copy = temp_copy.drop(columns=['geometry'])
        temp_copy.to_sql(feature.lower(), conn, if_exists="replace", index=False)

        # write out _in_fan features
        if not feature==selector:
            feature_name = feature + "_in_fan"
            temp_copy = data[feature_name].copy()
            temp_copy = temp_copy.drop(columns=['geometry'])
            temp_copy.to_sql(feature_name.lower(), conn, if_exists="replace", index=False)

    conn.close()

```

```{python}

output_folder = "../data"
os.makedirs(output_folder, exist_ok=True)

# Write each GeoDataFrame in the dictionary to a GeoJSON file
for key, gdf in data.items():
    if hasattr(gdf, "to_file"):  # Check it's a GeoDataFrame
        output_path = os.path.join(output_folder, f"{key}.geojson")
        gdf.to_file(output_path, driver="GeoJSON")
```



## Available columns

```{python}
#| output: asis

print(":::: {.columns} ")

width = round(floor(100.0 / len( [selector] + features )))

for feature in [selector] + features:
    if feature in features:
        feature = feature + "_in_fan"
    print(f"""
::: {{.column width={width}%}}
### {feature}

| Property |
|----------|""")

    columns = data[feature].columns.tolist()
    for col in columns:
        print(f"| {col} |")
    print (f"""

::: 
""")

print("\n::::")
```
